---
title: Covariance Matrix
author: John Lee
date: '2018-07-16'
slug: covariance-matrix
categories: []
tags:
  - theory
subtitle: ''
draft: false
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>In my first machine learning class, in order to learn about the theory behind PCA (Principal Component Analysis), we had to learn about variance-covariance matrix. I was concurrently taking a basic theoretical probability and statistics, so even the idea of variance was still vague to me. Despite the repeated attempts to understand covariance, I still had trouble fully capturing the intuition behind the covariance between two random variables. Even now, application and verification of correct usage of mathematical properties of covariance requires intensive googling. I want to use this post to review covariance and its properties.</p>
<p><strong>Basic Idea</strong> : Covariance of two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, also written as <span class="math inline">\(cov[X,Y]\)</span> and <span class="math inline">\(\sigma_{XY}\)</span>, is a strength of linear relationship between two random variables. It can be used to determine the directional strength of relationship between two random variables. Covariance is easily visualized by observing a distribution of bivariate normal distribution.</p>
<p>Consider a bivariate normal distribution (non degenerate) with random variables X and Y, where <span class="math inline">\(\mu_X, \mu_Y = 0\)</span>, <span class="math inline">\(\sigma_X, \sigma_Y = 1\)</span>, and <span class="math inline">\(\sigma_{XY}\)</span> takes different values.</p>
<pre class="r"><code>library(MASS)
library(mixtools)</code></pre>
<pre><code>## Warning: package &#39;mixtools&#39; was built under R version 4.0.5</code></pre>
<pre><code>## mixtools package, version 1.2.0, Released 2020-02-05
## This package is based upon work supported by the National Science Foundation under Grant No. SES-0518772.</code></pre>
<pre class="r"><code>set.seed(1234)
gen_binormplot &lt;- function(N, mu_vec, Sigma, ...){ #function that creates a plot for randomly generated bivariate normal distribution
  biv_norm &lt;- mvrnorm(
    n = N,
    mu = mu_vec,
    Sigma = Sigma
  )
  plot(x = biv_norm[, 1],
       y = biv_norm[, 2],
       main = bquote(&#39;bivariate normal distribution when&#39; ~ sigma[&#39;xy&#39;] == .(Sigma[2])),
       xlab = &#39;x&#39;,
       ylab = &#39;y&#39;,
       col = &#39;dark grey&#39;,
       ...)
  for (ellipse_alpha in seq(0, 1, 0.2)){
    ellipse(mu_vec,
            Sigma,
            alpha = ellipse_alpha,
            type = &#39;l&#39;,
            col = &#39;red&#39;,
            lwd = 1.5)
  }
}

N &lt;- 1000
mu_x &lt;- 0; mu_y &lt;- 0
var_x &lt;- 1; var_y &lt;- 1
cov_xy &lt;- c(-0.9, 0.9, -0.7, 0.7, -0.3, 0.3, 0)

par(mfrow = c(2,2), mai = c(0.5, 0.5, 0.5, 0.5))
for (plt in 1:length(cov_xy)){
  Sig &lt;- matrix(c(var_x, cov_xy[plt], cov_xy[plt], var_y), nrow = 2, byrow = TRUE)
  gen_binormplot(N, c(mu_x, mu_y), Sig, xlim = c(-3, 3), ylim = c(-3, 3))
}</code></pre>
<p><img src="/post/2018-07-16-covariance-matrix_files/figure-html/unnamed-chunk-1-1.png" width="672" /><img src="/post/2018-07-16-covariance-matrix_files/figure-html/unnamed-chunk-1-2.png" width="672" />
The ellipses show the region in which a certain portion of the points from a bivariate normal distribution falls into. The outermost ellipse contains 95% of the points. Also, the ellipses allows us to see more easily the general trend that the two random variables have. As you can see, <em>negative covariance is associated with inverse relationship between X and Y</em>, and <em>positive covariance is associated with direct relationship</em>. Furthermore, the ellipses tend to get wider as the absolute value of covariance decreases. This is because lower absoluate value of covariance is regarded as the two random variables having a “weaker” relationship. In fact, the ellipses in the case where <span class="math inline">\(\sigma_{XY} = 0\)</span> would be a perfect circle had the plot size been square (which is also ascribed to the variance of X and Y being equal).</p>
<p><strong>Mathematical Definition</strong>: <span class="math inline">\(Cov[X, Y] = E\big[(X-E[X])(Y-E[Y])\big]\)</span></p>
<p>In words, the covariance of random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is an expectation of the product of the two variables’ deviation from the mean.</p>
<p>From this definition, I like to understand the previously seen trend (where negative covariance is related to inverse relationship and positive covariance is related to direct relationship) by thinking of extreme cases. If X and Y were to have inverse (linear) relationship, where increasing X is related to decreasing Y, then higher values of X, which is above its mean will be paired with lower values of Y, which is below the mean. Then the deviation for X and Y will have opposite signs. So the expected value of their product would be negative. Similar reason applies for positive covariance.</p>
<p>Covariance can also be written as: <span class="math inline">\(Cov[X, Y] = E[XY] - E[X]E[Y]\)</span>.</p>
<p>With this formula, it is easier to see why two independent random variables have covariance of 0, since for two independent random variables X and Y, <span class="math inline">\(E[XY] = E[X]E[Y]\)</span>. However, the converse is not always true.</p>
<p>A simple example:</p>
<p><span class="math inline">\(X \sim unif(-1, 1)\)</span></p>
<p><span class="math inline">\(Y = X^2\)</span></p>
<p>Then <span class="math inline">\(Cov[X, Y] = E[XY] - E[X]E[Y] = E[X^3] - E[X]E[X^2] = 0 - 0 \times E[X^2] = 0\)</span>, but X and Y are clearly dependent.</p>
<p>Plot of samples from these two random variables will look like</p>
<pre class="r"><code>x &lt;- runif(n = 100, min = -1, max = 1); y &lt;- x^2
plot(x = x, y = y, xlim = c(-1.5, 1.5), ylim = c(-0.5, 1.5), xlab = &quot;X&quot;, ylab = &quot;Y&quot;)</code></pre>
<p><img src="/post/2018-07-16-covariance-matrix_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Clearly the relationship between the two random variables is not linear, thus covariance alone would not be a good method to measure the strength of relationship between the two random variables.</p>
<p>The strength of linear relationship between two random variables is hard to determine by solely looking at the covariance because the value depends on how the random variables are scaled. Hence the covariance is often normalized into Pearson correlation coefficient (<span class="math inline">\(\rho\)</span>), which ranges from -1 to 1, so that it is easier to interpret.</p>
<p>The definition of correlation coefficient is given by: <span class="math display">\[\rho_{_{XY}} = \dfrac{Cov[X,Y]}{Sd[X]Sd[Y]} = \dfrac{\sigma_{_{XY}}}{\sigma_{_X}\sigma_{_Y}}\]</span>
where <span class="math inline">\(Sd\)</span> is standard deviation. This is easier to interpret than covariance because it is scaled down to in between -1 and 1. Closer absolute value to 1 means that the linear relationship is stronger.</p>
<p><strong>Properties of covariance</strong></p>
<p><span class="math inline">\(Cov[X, a] = 0\)</span></p>
<p>where a is a constant. In words covariance between a random variable and a constant is 0.</p>
<p><span class="math inline">\(Cov[X,X] = Var[X]\)</span></p>
<p>Covariance of a random variable with itself is a variance.</p>
<p><span class="math inline">\(Cov[X,Y] = Cov[Y,X]\)</span></p>
<p>Covariance of two random variables is the same when the order is flipped. This property leads to the variance-covariance matrix being symmetric</p>
<p><span class="math inline">\(Cov[aX,bY] = abCov[X,Y]\)</span></p>
<p>This property tells me that covariance multiplicately scales corresponding to the changes in the random variables X and Y. This is due to linearity of expectation. The coefficient for each random variable can be pulled out individually because covariance is an expected value of the <em>product</em> of each random variables. Different scaling doesn’t change the strength of relationship, even though higher covariance deceptively indicates so (I could be the only person that had this misconception).</p>
<pre class="r"><code>N &lt;- 1000
mu_x &lt;- 5; mu_y &lt;- 5
var_x &lt;- 1; var_y &lt;- 1
cov_xy &lt;- 0.7

bivnorm &lt;- mvrnorm(n = N, mu = c(mu_x, mu_y), Sigma = matrix(c(var_x, cov_xy, cov_xy, var_y), nrow = 2, byrow = TRUE))
par(mfrow = c(1, 2))
plot(bivnorm[,1], bivnorm[,2], xlim = c(1, 9), ylim = c(1, 9), xlab = &#39;X&#39;, ylab = &#39;Y&#39;)
plot(10*bivnorm[,1], bivnorm[,2], xlim = c(10, 90), ylim = c(1, 9), xlab = &#39;10 * X&#39;, ylab = &#39;Y&#39;)</code></pre>
<p><img src="/post/2018-07-16-covariance-matrix_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p><span class="math inline">\(Cov[X + a, Y + b] = Cov[X,Y]\)</span></p>
<p>Like variance, covariance is invariant to locational shifts</p>
<p><span class="math inline">\(Cov[aX + bY,cW + dZ] = acCov[X,W] + bcCov[Y,W] + adCov[X,Z] + bdCov[Y,Z]\)</span></p>
<p>If each coordinate is a linear combination of random variables, then each random variable ‘term’ can be taken out individually. Hence the covariance is equivalent to the summation of the covariance of each ‘pair’ of random variables.</p>
<p><strong>Sample Covariance</strong></p>
<p>Usually population parameters will be unknown and I’ll have to use samples to calculate the covariance. The formula for sample covariance is as follows:</p>
<p><span class="math display">\[Cov[X,Y] = \dfrac{1}{n-1}\sum_{i=i}^{n}(X_i - \bar{X})(Y_i - \bar{Y})\]</span></p>
<p>This <a href="https://stats.seandolinar.com/covariance-different-ways-to-explain/">blog</a> translates the calculation of covariance to geometrical sense nicely.</p>
<p><strong>Moving to variance-covariance matrix</strong></p>
<p>Variance-covariance matrix allows us to see covariance of all pairs of components in a random vector. For a random vector <span class="math inline">\(\textbf{X}\)</span> of length <span class="math inline">\(N\)</span>, the variance-covariance matrix of a random vector X (denote as <span class="math inline">\(Var(\textbf{X})\)</span>, <span class="math inline">\(Cov(\textbf{X})\)</span>, <span class="math inline">\(\Sigma_\textbf{X}\)</span>) is an <span class="math inline">\(N \times N\)</span> matrix whose <span class="math inline">\(i^{th}\)</span> row and <span class="math inline">\(j^{th}\)</span> is a covariance between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span>.</p>
<p><span class="math inline">\(\textbf{X} = \begin{pmatrix} X_1 \\ \vdots \\ X_N \end{pmatrix}\)</span></p>
<p><span class="math inline">\(\Sigma_\textbf{X} = \begin{pmatrix} \sigma_{11} &amp; \sigma_{12} &amp; \cdots &amp; \sigma_{1N} \\ \sigma_{21} &amp; \sigma_{22} &amp; \cdots &amp; \sigma_{2N} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \sigma_{N1} &amp; \sigma_{N2} &amp; \cdots &amp; \sigma_{NN} \end{pmatrix}\)</span></p>
<p>Applying definition of covariance, the <span class="math inline">\(i^{th}\)</span> row and <span class="math inline">\(j^{th}\)</span> column is <span class="math inline">\(\Sigma_{ij} = E\big[(X_i-E[X_i])(X_j-E[X_j])\big]\)</span>.</p>
<p><strong>The following are some properties of <span class="math inline">\(\text{ } \Sigma\)</span> :</strong></p>
<p>The <span class="math inline">\(i^{th}\)</span> diagonal entry is a variance of an <span class="math inline">\(i^{th}\)</span> random varible.
f
<span class="math inline">\(\Sigma_\textbf{X}\)</span> is a symmetric matrix due to the third property of covariance listed above.</p>
<p><span class="math inline">\(\Sigma_\textbf{X}\)</span> is positive semi-definite. In other words, for <span class="math inline">\(\text{ } \textbf{v} \in \mathbb{R}^N\)</span>,
<span class="math display">\[\textbf{v}^T \Sigma_\textbf{X} \textbf{v} \geq 0\]</span>
I’ll provide a simple proof as practice:</p>
<p><span class="math display">\[\textbf{v}^T \Sigma_\textbf{X} \textbf{v} = \textbf{v}^TE\big[(\textbf{X}-E[\textbf{X}])(\textbf{X}-E[\textbf{X}])^T\big]\textbf{v} \]</span>
<span class="math display">\[= E\big[\textbf{v}^T(\textbf{X}-E[\textbf{X}])(\textbf{X}-E[\textbf{X}])^T\textbf{v}\big] \]</span>
<span class="math display">\[= E\big[\textbf{v}^T (\textbf{X}-E[\textbf{X}])(\textbf{v}^T(\textbf{X}-E[\textbf{X}]))^T\big] \]</span>
<span class="math display">\[= E\big[ \big( \textbf{v}^T(\textbf{X}-E[\textbf{X}]) \big)^2 \big] \geq 0 \text{, } \forall \text{ } \textbf{v} \in \mathbb{R}^N\]</span>
<em>Note: In the case that <span class="math inline">\(\exists \text{ } \textbf{v} \in \mathbb{R}^N \text{ s.t. } \textbf{v}^T\Sigma\vec{v} = 0\)</span>, there is a component in a random vector <span class="math inline">\(X\)</span> that can be expressed through linear combination of other components </em></p>
<p><span class="math inline">\(\Sigma_{A\textbf{X}+b} = A\Sigma_\textbf{X}A^T\)</span></p>
<p>Correlation matrix, (denote as <span class="math inline">\(\vec{\rho}\)</span>) whose <span class="math inline">\(i^{th}\)</span> row and <span class="math inline">\(j^{th}\)</span> column expresses the correlation coefficient between the respective random variables, can be obtained from the variance-covariance matrix.</p>
<p>We know that <span class="math inline">\(\vec{\rho}_{ij} = \dfrac{Cov(X_i, X_j)}{\sqrt{Var(X_i)Var(X_j)}}\)</span> and the diagnonal entries of <span class="math inline">\(\text{ } \Sigma \text{ }\)</span> are variances. Hence, <span class="math inline">\(\vec{\rho} = \big(diag(\Sigma)\big)^{-\frac{1}{2}} \Sigma \big(diag(\Sigma)\big)^{-\frac{1}{2}}\)</span>, where <span class="math inline">\(diag(\Sigma)\)</span> is a diagonal matrix whose diagonal entries are diagonal entries of <span class="math inline">\(\Sigma\)</span> and other entries 0.</p>
<p><strong>Calculating variance-covariance matrix from a sample</strong></p>
<p>Suppose we have a set of data whose <span class="math inline">\(\text{ }N\text{ }\)</span>observations are each row and <span class="math inline">\(\text{ }p\text{ }\)</span> features (variables) are each column. Denote the data by <span class="math inline">\(\text{ }D\)</span>. To calculate for variance-covariance matrix, the values first need to be centered around their mean, then multiplied and summed accordingly.</p>
<p>Let <span class="math inline">\(\textbf{D}_c\)</span> be data whose values are centered around their mean. Then variance-covariance matrix <span class="math inline">\(V\)</span> is:</p>
<p><span class="math inline">\(V = \dfrac{1}{N-1} \textbf{D}^T_c\textbf{D}_c\)</span></p>
<p>Example using <code>attitude</code> dataset:</p>
<pre class="r"><code>summary(attitude)</code></pre>
<pre><code>##      rating        complaints     privileges       learning         raises     
##  Min.   :40.00   Min.   :37.0   Min.   :30.00   Min.   :34.00   Min.   :43.00  
##  1st Qu.:58.75   1st Qu.:58.5   1st Qu.:45.00   1st Qu.:47.00   1st Qu.:58.25  
##  Median :65.50   Median :65.0   Median :51.50   Median :56.50   Median :63.50  
##  Mean   :64.63   Mean   :66.6   Mean   :53.13   Mean   :56.37   Mean   :64.63  
##  3rd Qu.:71.75   3rd Qu.:77.0   3rd Qu.:62.50   3rd Qu.:66.75   3rd Qu.:71.00  
##  Max.   :85.00   Max.   :90.0   Max.   :83.00   Max.   :75.00   Max.   :88.00  
##     critical        advance     
##  Min.   :49.00   Min.   :25.00  
##  1st Qu.:69.25   1st Qu.:35.00  
##  Median :77.50   Median :41.00  
##  Mean   :74.77   Mean   :42.93  
##  3rd Qu.:80.00   3rd Qu.:47.75  
##  Max.   :92.00   Max.   :72.00</code></pre>
<pre class="r"><code>att_c &lt;- sapply(attitude, function(x){x-mean(x)})
cov_att_manual &lt;- 1/(nrow(att_c) - 1) * (t(att_c) %*% att_c)
cov_att &lt;- cov(attitude)

print(cov_att)</code></pre>
<pre><code>##               rating complaints privileges  learning    raises critical
## rating     148.17126  133.77931   63.46437  89.10460  74.68851 18.84253
## complaints 133.77931  177.28276   90.95172  93.25517  92.64138 24.73103
## privileges  63.46437   90.95172  149.70575  70.84598  56.67126 17.82529
## learning    89.10460   93.25517   70.84598 137.75747  78.13908 13.46782
## raises      74.68851   92.64138   56.67126  78.13908 108.10230 38.77356
## critical    18.84253   24.73103   17.82529  13.46782  38.77356 97.90920
## advance     19.42299   30.76552   43.21609  64.19770  61.42299 28.84598
##              advance
## rating      19.42299
## complaints  30.76552
## privileges  43.21609
## learning    64.19770
## raises      61.42299
## critical    28.84598
## advance    105.85747</code></pre>
<pre class="r"><code>print(all.equal(cov_att_manual, cov_att))</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>Although there will almost never be an occasion where I have to calculate the covariance matrix or even other statistics by hand, I thought it would be a good way to practice with matrices and to keep myself aware of the mathematical end of statistics.</p>
<p>There is also another type of covariance matrix called cross-covariance matrix for <em>two</em> random vectors, but I will not go beyond writing the definition.</p>
<p><span class="math display">\[cov(\textbf{X},\textbf{Y}) = E\big[(\textbf{X}-E[\textbf{X}])(\textbf{Y}-E[\textbf{Y}])\big]\]</span></p>
<p>There is a lot more to covariance and variance-covariance matrix, but this should be enough to found a solid ground for understanding covariance.</p>
