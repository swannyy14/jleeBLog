---
title: First Post
author: John Lee
date: '2018-07-06'
slug: first-post
categories: []
tags: []
subtitle: ''
---

This is the first blog post of my life! I will be using this blog to post about anything that I want to share in statistics. For starter, I will run a linear regression with the `iris` dataset.


```{r}
names(iris)
```
Let's predict `Sepal.Length` with `Petal.Length` and `Petal.Width`.

```{r}
#separate into training and testing sets
set.seed(1234)
train_ind <- sample(nrow(iris), floor(0.8 * nrow(iris)))
iris_train <- iris[train_ind,]
iris_test <- iris[-train_ind,]

#run linear regression
iris_lm <- lm(Sepal.Length ~ Petal.Length + Petal.Width, data = iris_train)
summary(iris_lm)
```

The corresponding equation for `iris_lm` model is
$$\hat{y}_i = `r round(iris_lm$coefficients[1], 2)` + `r round(iris_lm$coefficients[2], 2)`x_{1i}  `r round(iris_lm$coefficients[3], 2)`x_{2i}$$
Although all the coefficients show significance in terms of p-value, let's create diagnostic plots to check if the model fits correctly. 
```{r}
par(mfrow = c(2, 2))
plot(iris_lm)
```

There seems to be a nonlinear trend in the `Residuals vs Fitted` diagnostic plot. This suggests that the true model may not be linear. As a result, I will include a quadratic term for the predictors.

The resulting quadratic model will take the form of
$$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_{11}x_{1i} + \hat{\beta}_{12}x_{1i}^2 + \hat{\beta}_{21}x_{2i} + \hat{\beta}_{22}x_{2i}^2$$

```{r}
iris_lm_quad <- lm(Sepal.Length ~ poly(Petal.Length, 2, raw = TRUE) + poly(Petal.Width, 2, raw = TRUE),
                   data = iris_train)
par(mfrow = c(2, 2))
plot(iris_lm_quad)
summary(iris_lm_quad)
```

Now the non-linearity problem is gone! It looks as though there is an observation with leverage that is noticeably higher than that of others, but the observation has the Cook's Distance well below 0.5; so I will not investigate further.
Judging from the summary, it is evident that the quadratic polynomial for `Petal.Width` is not very significant. Let's remove that predictor and see how the model turns out.

```{r}
iris_lm_quad <- update(iris_lm_quad, Sepal.Length ~ poly(Petal.Length, 2, raw = TRUE) + Petal.Width)
summary(iris_lm_quad)
```

Now all the predictors are significant with the exception for the first order polynomial for `Petal.Length`. Nonetheless, I have to respect the hierarchy of higher order terms, so I'll keep the coefficient.

The summary suggests that the model is statistically significant, and has the R-squared value of roughly 0.8. I'll use this model to predict the `Sepal.Length` with the test dataset created in the beginning of this post.

```{r}
y <- iris_test[,"Sepal.Length"]
y_hat <- predict(iris_lm_quad, iris_test[,c("Petal.Length", "Petal.Width")])

#calculate Root Mean Squared Error
rmse <- sqrt(sum((y - y_hat)^2)/length(y))
```

I used Root Mean Square Error to quantify the accuracy of the model, and it came out to be `r round(rmse, 3)`.

This marks the end of my first post. As I continue this blog, I hope to learn statistics and the models more in depth and become more adept at coming up with creative solutions for data analysis problems!